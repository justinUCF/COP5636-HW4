{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Homework 4: Petting a warg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Petting a warg\n",
    "\n",
    "Wargs do not make good pets. They are vicious creatures, populating Middle Earth, the world described by novels of John Ronald Reuel Tolkien. They tend to show up in the worst moment possible. They eat humans, hobbits, elves and wizards (when they can get them).\n",
    "\n",
    "![A warg, getting ready for breakfast w:300px](figures/Gundabad_Wargs.jpg)\n",
    "\n",
    "Your relationship with a warg can be in the following states:\n",
    "```\n",
    "SleepingWarg\n",
    "AngryWarg\n",
    "FuriousWarg\n",
    "ApoplecticWarg\n",
    "Safe\n",
    "Sorry \n",
    "```\n",
    "\n",
    "![tes](figures/WargStates.jpg)\n",
    "\n",
    "Your actions are limited to petting a warg or striking it with your sword. The transitions are described in the following picture. The safe and sorry states are terminal, where no further actions can be taken. Landing into them has the reward +10 and -10 respectively. All other actions have a reward of -1. \n",
    "\n",
    "The discount factor is $\\gamma=0.9$\n",
    "\n",
    "![](figures/PetAWarg.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to solve this homework\n",
    "The following problems you can solve either with the help of an LLM or by hand. \n",
    "\n",
    "* If you are solving by hand, make sure that you add sufficient comments to make sure that the code is understandable. \n",
    "* If you are solving using an LLM, add in form of comments\n",
    "    * the LLM used (at the first use instance)\n",
    "    * the prompt used to elicit the code\n",
    "    * modifications that had to be done to the code \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "# --- LLM used: ChatGPT 4.5\n",
    "# --- LLM prompt\n",
    "# Write a python class to encapsulate the least common multiple algorithm\n",
    "# --- End of LLM prompt\n",
    "```\n",
    "\n",
    "The programming language should be Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1: MDP implementation \n",
    "\n",
    "Write a class to implement an MDP. Do not include value or policy iteration in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    def __init__(self, states, terminal_states, actions, discount_factor):\n",
    "\n",
    "        self.states = states\n",
    "        self.start_state = states[0]\n",
    "        self.actions = actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.terminal_states = terminal_states\n",
    "        # --- LLM used: Claude Sonnet 4.5\n",
    "        # --- LLM prompt: \n",
    "        # What is a way I can implement a set of transition functions without having to pass them into the class\n",
    "        # to ensure I can store the probabiulity and reward in the same structure?\n",
    "        self.transitions = {s: {a: [] for a in actions} for s in states}\n",
    "        \n",
    "    def add_transition(self, state, action, info: tuple[float, str, int]):\n",
    "        probability, next_state, reward = info\n",
    "        self.transitions[state][action].append((probability, next_state, reward))\n",
    "    # --- End of LLM prompt\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return (state in self.terminal_states)\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transitions[state][action]\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor\n",
    "    \n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "    \n",
    "    def get_terminal_states(self):\n",
    "        return self.terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2: Warg as an MDP\n",
    "Implement the WargPettingGame as an MDP using the implementation from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WargPettingGame_MDP():\n",
    "    states = ['SleepingWarg', 'AngryWarg', 'FuriousWarg', 'ApoplecticWarg']\n",
    "    terminal_states = ['Safe', 'Sorry']\n",
    "    actions = ['Strike', 'Pet']\n",
    "    discount_factor = 0.9\n",
    "\n",
    "    game = MDP(states, terminal_states, actions, discount_factor)\n",
    "    # SleepingWarg State\n",
    "    game.add_transition('SleepingWarg', 'Strike', (1.0, 'AngryWarg', -1))\n",
    "    game.add_transition('SleepingWarg', 'Pet', (0.95, 'AngryWarg', -1))\n",
    "    game.add_transition('SleepingWarg', 'Pet', (0.05, 'Safe', 10))\n",
    "    # AngryWarg State\n",
    "    game.add_transition('AngryWarg', 'Strike', (1.0, 'FuriousWarg', -1))\n",
    "    game.add_transition('AngryWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "    # FuriousWarg State\n",
    "    game.add_transition('FuriousWarg', 'Strike', (1.0, 'ApoplecticWarg', -1))\n",
    "    game.add_transition('FuriousWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "    # ApoplecticWarg\n",
    "    game.add_transition('ApoplecticWarg', 'Strike', (0.8, 'Sorry', -10))\n",
    "    game.add_transition('ApoplecticWarg', 'Strike', (0.2, 'Safe', 10))\n",
    "    game.add_transition('ApoplecticWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3: Value iteration\n",
    "\n",
    "Implement the value iteration as a separate function that uses this MDP implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: MDP, iterations=10000, convergence=0.0001):\n",
    "    # Inilialize V for every state to 0.0\n",
    "    V = {s: 0.0 for s in mdp.get_states()} \n",
    "    for terminal_state in mdp.get_terminal_states():\n",
    "        V[terminal_state] = 0.0\n",
    "    for i in range(iterations):\n",
    "        V_next = V.copy() # V_k+1\n",
    "        delta = 0 # Difference between iterations for convergence\n",
    "\n",
    "        # --- LLM used: Claude Sonnet 4.5\n",
    "        # --- LLM prompt\n",
    "        # How can I loop through each of the possible transitions for a given action and compute the q value for \n",
    "        # value iteration. Please review these slides from my class to understand the bellman equation we used.\n",
    "\n",
    "        # For each non-terminal state\n",
    "        for state in mdp.get_states():\n",
    "\n",
    "            # Compute Q*(s,a) for each action\n",
    "            action_values = []\n",
    "            for action in mdp.get_actions(state):\n",
    "                q = 0\n",
    "\n",
    "                # Q*(s,a) = Σ T(s,a,s') * [R(s,a,s') + γ * V_k(s')]\n",
    "                transitions = mdp.get_transitions(state, action)\n",
    "                for info in transitions:\n",
    "                    probability, next_state, reward = info\n",
    "                    q += probability * (reward + mdp.get_discount_factor() * V[next_state])\n",
    "\n",
    "                action_values.append(q)\n",
    "\n",
    "            # V_{k+1}(s) = max_a Q*(s,a) (Bellman update)\n",
    "            if action_values:\n",
    "                V_next[state] = max(action_values)\n",
    "                delta = max(delta, abs(V_next[state] - V[state]))\n",
    "\n",
    "        V = V_next\n",
    "        # --- End of LLM prompt\n",
    "        if delta < convergence:\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4: Using value iteration\n",
    "Find the V* values of the WargPettingGame using the implementation above. Print out the V* values for each state in the form \n",
    "V(state) == number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(SleepingWarg) = -6.2298\n",
      "V(AngryWarg) = -6.760000000000001\n",
      "V(FuriousWarg) = -6.4\n",
      "V(ApoplecticWarg) = -6.0\n",
      "V(Safe) = 0.0\n",
      "V(Sorry) = 0.0\n"
     ]
    }
   ],
   "source": [
    "mdp = WargPettingGame_MDP()\n",
    "V_star = value_iteration(mdp)\n",
    "for state, action in V_star.items():\n",
    "    print(f\"V({state}) = {action}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5:  Policy extraction\n",
    "\n",
    "Find the policy $\\pi(s)$ from the V values obtained in the previous step. Remember that you need to do one step of expectimax.\n",
    "Print out the policy for each state, in a readable way. Eg. \n",
    "    pi(ApoplecticWarg) = Pet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi(SleepingWarg) = Pet\n",
      "pi(AngryWarg) = Strike\n",
      "pi(FuriousWarg) = Strike\n",
      "pi(ApoplecticWarg) = Strike\n"
     ]
    }
   ],
   "source": [
    "def extract_policy(mdp: MDP, V: dict):\n",
    "    policy = {}\n",
    "    \n",
    "    for state in mdp.get_states():\n",
    "        policy[state] = None\n",
    "            \n",
    "        \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        # Try each action\n",
    "        for action in mdp.get_actions(state):\n",
    "            q_value = 0\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            \n",
    "            # Compute expected value for this action\n",
    "            for prob, next_state, reward in transitions:\n",
    "                q_value += prob * (reward + mdp.get_discount_factor() * V[next_state])\n",
    "            \n",
    "            # Update best action if this is better\n",
    "            if q_value > best_value:\n",
    "                best_value = q_value\n",
    "                best_action = action\n",
    "        \n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy\n",
    "\n",
    "policy = extract_policy(mdp, V_star)\n",
    "for state, action in policy.items():\n",
    "    print(f\"pi({state}) = {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P6: Policy iteration\n",
    "Implement policy iteration with the MDP as defined above as a separate function.\n",
    "Apply it to the MDP defining the pet the warg game. \n",
    "Print out the resulting policy for each state, in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Result:\n",
      "Policy iteration converged in 2 iterations\n",
      "π(SleepingWarg) = Pet\n",
      "π(AngryWarg) = Strike\n",
      "π(FuriousWarg) = Strike\n",
      "π(ApoplecticWarg) = Strike\n"
     ]
    }
   ],
   "source": [
    "# --- LLM used: Claude Sonnet 4.5\n",
    "# --- LLM prompt: \n",
    "# Please help me produce policy iteration based on the previous MDP class provided. I have already implemented policy\n",
    "# extraction with the fucntion extract_policy(mdp: MDP, V: dict) that you may call. Please set the max iterations attempt\n",
    "# to 10000 and convergence to 0.0001 then print the results of the policy iteration function.\n",
    "def policy_evaluation(mdp: MDP, policy: dict, convergence=0.0001):\n",
    "    \"\"\"\n",
    "    Evaluate a fixed policy to find V^π.\n",
    "    Similar to value_iteration but follows policy instead of taking max.\n",
    "    \"\"\"\n",
    "    # Initialize V for all states\n",
    "    V = {s: 0.0 for s in mdp.get_states()}\n",
    "    for terminal_state in mdp.get_terminal_states():\n",
    "        V[terminal_state] = 0.0\n",
    "    \n",
    "    for i in range(10000):  # Max iterations\n",
    "        V_next = V.copy()\n",
    "        delta = 0\n",
    "        \n",
    "        for state in mdp.get_states():\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            # KEY DIFFERENCE: Use policy's action, don't take max\n",
    "            action = policy[state]\n",
    "            if action is None:\n",
    "                continue\n",
    "            \n",
    "            # Compute value for this specific action\n",
    "            v = 0\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            for prob, next_state, reward in transitions:\n",
    "                v += prob * (reward + mdp.get_discount_factor() * V[next_state])\n",
    "            \n",
    "            V_next[state] = v\n",
    "            delta = max(delta, abs(V_next[state] - V[state]))\n",
    "        \n",
    "        V = V_next\n",
    "        \n",
    "        if delta < convergence:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_iteration(mdp: MDP, max_iterations=10000):\n",
    "    \"\"\"Policy iteration using existing extract_policy function.\"\"\"\n",
    "    # Initialize with random policy\n",
    "    policy = {}\n",
    "    for state in mdp.get_states():\n",
    "        actions = mdp.get_actions(state)\n",
    "        policy[state] = actions[0] if actions else None\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Step 1: Evaluate current policy\n",
    "        V = policy_evaluation(mdp, policy)\n",
    "        \n",
    "        # Step 2: Improve policy (reuse your extract_policy!)\n",
    "        new_policy = extract_policy(mdp, V)\n",
    "        \n",
    "        # Check convergence\n",
    "        if new_policy == policy:\n",
    "            print(f\"Policy iteration converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy\n",
    "\n",
    "# Run it\n",
    "print(\"Policy Iteration Result:\")\n",
    "policy_iter_result = policy_iteration(mdp)\n",
    "for state, action in policy_iter_result.items():\n",
    "    print(f\"π({state}) = {action}\")\n",
    "\n",
    "# --- End LLM prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7: Trajectory sampling\n",
    "Implement a function that generates trajectories in the form of (s,a,r,s') tuples from the MDP for a specific policy. The trajectory ends when it reaches a terminal state. \n",
    "\n",
    "Generate 100 trajectories for a __random__ policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 100 random trajectories...\n",
      "Generated 100 trajectories\n",
      "Example trajectory 1 (length 4):\n",
      "  SleepingWarg --[Strike, r=-1]--> AngryWarg\n",
      "  AngryWarg --[Strike, r=-1]--> FuriousWarg\n",
      "  FuriousWarg --[Strike, r=-1]--> ApoplecticWarg\n",
      "  ApoplecticWarg --[Pet, r=-10]--> Sorry\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# --- LLM used: Claude Sonnet 4.5\n",
    "# --- LLM prompt:\n",
    "# I need to implement trajectory sampling for an MDP. The function should generate trajectories \n",
    "# in the form of (state, action, reward, next_state) tuples by simulating the MDP. \n",
    "# The trajectory should start from the start_state and continue until reaching a terminal state.\n",
    "# I need to handle two types of policies: a dictionary mapping states to actions, or 'random' \n",
    "# which chooses actions randomly. For stochastic transitions, sample the next state according \n",
    "# to the transition probabilities using numpy.random.choice.\n",
    "# Also create a function to generate multiple trajectories.\n",
    "def generate_trajectory(mdp: MDP, policy, start_state=None):\n",
    "    \"\"\"\n",
    "    Generate a single trajectory following a policy until reaching terminal state.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object\n",
    "        policy: Either a dict mapping states to actions, or 'random' for random policy\n",
    "        start_state: Starting state (if None, uses mdp.start_state)\n",
    "    \n",
    "    Returns:\n",
    "        List of (state, action, reward, next_state) tuples\n",
    "    \"\"\"\n",
    "    trajectory = []\n",
    "    current_state = start_state if start_state else mdp.start_state\n",
    "    \n",
    "    while not mdp.is_terminal(current_state):\n",
    "        # Choose action based on policy\n",
    "        if policy == 'random':\n",
    "            actions = mdp.get_actions(current_state)\n",
    "            if not actions:\n",
    "                break\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = policy.get(current_state)\n",
    "            if action is None:\n",
    "                break\n",
    "        \n",
    "        # Get possible transitions for this state-action pair\n",
    "        transitions = mdp.get_transitions(current_state, action)\n",
    "        \n",
    "        # Sample next state according to transition probabilities\n",
    "        probs = [t[0] for t in transitions]\n",
    "        next_states = [t[1] for t in transitions]\n",
    "        rewards = [t[2] for t in transitions]\n",
    "        \n",
    "        # Sample based on probabilities\n",
    "        idx = np.random.choice(len(transitions), p=probs)\n",
    "        next_state = next_states[idx]\n",
    "        reward = rewards[idx]\n",
    "        \n",
    "        # Record transition\n",
    "        trajectory.append((current_state, action, reward, next_state))\n",
    "        current_state = next_state\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def generate_trajectories(mdp: MDP, policy, num_trajectories=100, start_state=None):\n",
    "    \"\"\"Generate multiple trajectories.\"\"\"\n",
    "    trajectories = []\n",
    "    for _ in range(num_trajectories):\n",
    "        traj = generate_trajectory(mdp, policy, start_state)\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "# --- End LLM prompt\n",
    "\n",
    "# Generate 100 trajectories with random policy\n",
    "print(\"\\nGenerating 100 random trajectories...\")\n",
    "random_trajectories = generate_trajectories(mdp, 'random', num_trajectories=100)\n",
    "\n",
    "print(f\"Generated {len(random_trajectories)} trajectories\")\n",
    "print(f\"Example trajectory 1 (length {len(random_trajectories[0])}):\")\n",
    "for step in random_trajectories[0]:\n",
    "    state, action, reward, next_state = step\n",
    "    print(f\"  {state} --[{action}, r={reward}]--> {next_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P8: Implement Q-learning \n",
    "\n",
    "Create an implementation of Q-learning which takes the trajectory database and updates a Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# --- LLM used: Claude Sonnet 4.5\n",
    "# --- LLM prompt:\n",
    "# Implement Q-learning that takes a database of trajectories and updates a Q-table.\n",
    "# The Q-table should be initialized with a default value for all state-action pairs.\n",
    "# For each (state, action, reward, next_state) tuple in the trajectories, apply the Q-learning\n",
    "# use the update rule for Q from the provided powerpoint slides.\n",
    "# Use collections.defaultdict for the Q-table structure.\n",
    "def q_learning(trajectories, mdp: MDP, alpha=0.1, initial_q=0.0):\n",
    "    \"\"\"\n",
    "    Perform Q-learning on a database of trajectories.\n",
    "    \n",
    "    Args:\n",
    "        trajectories: List of trajectories (each is list of (s,a,r,s') tuples)\n",
    "        mdp: MDP object\n",
    "        alpha: Learning rate\n",
    "        initial_q: Initial Q-value for all state-action pairs\n",
    "    \n",
    "    Returns:\n",
    "        Q-table as nested dictionary Q[state][action]\n",
    "    \"\"\"\n",
    "    # Initialize Q-table\n",
    "    Q = defaultdict(lambda: defaultdict(lambda: initial_q))\n",
    "    \n",
    "    # Initialize all state-action pairs explicitly\n",
    "    for state in mdp.get_states():\n",
    "        for action in mdp.get_actions(state):\n",
    "            Q[state][action] = initial_q\n",
    "    \n",
    "    # Process all trajectories\n",
    "    for trajectory in trajectories:\n",
    "        for state, action, reward, next_state in trajectory:\n",
    "            # Get max Q-value for next state\n",
    "            if mdp.is_terminal(next_state):\n",
    "                max_q_next = 0.0\n",
    "            else:\n",
    "                next_actions = mdp.get_actions(next_state)\n",
    "                if next_actions:\n",
    "                    max_q_next = max([Q[next_state][a] for a in next_actions])\n",
    "                else:\n",
    "                    max_q_next = 0.0\n",
    "            \n",
    "            # Q-learning update rule:\n",
    "            # Q(s,a) ← Q(s,a) + α * [r + γ * max_a' Q(s',a') - Q(s,a)]\n",
    "            Q[state][action] = Q[state][action] + alpha * (\n",
    "                reward + mdp.get_discount_factor() * max_q_next - Q[state][action]\n",
    "            )\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# --- End LLM prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P9: Run Q-learning \n",
    "\n",
    "Run your implementation of Q-learning on the warg petting game. Print out the Q values in the form \n",
    "\n",
    "Q(state, action) = number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Q-Learning...\n",
      "\n",
      "Q-values:\n",
      "Q(SleepingWarg, Strike) = -9.0271\n",
      "Q(SleepingWarg, Pet) = -8.3494\n",
      "Q(AngryWarg, Strike) = -8.9077\n",
      "Q(AngryWarg, Pet) = -10.0000\n",
      "Q(FuriousWarg, Strike) = -8.8436\n",
      "Q(FuriousWarg, Pet) = -10.0000\n",
      "Q(ApoplecticWarg, Strike) = -9.0422\n",
      "Q(ApoplecticWarg, Pet) = -10.0000\n"
     ]
    }
   ],
   "source": [
    "# --- LLM used: Claude Sonnet 4.5\n",
    "# --- LLM prompt:\n",
    "# Create a function to run Q-learning on the Warg MDP. The function should:\n",
    "# 1. Generate multiple episodes of trajectories using a random policy\n",
    "# 2. Run Q-learning on the collected trajectories\n",
    "# 3. Print the resulting Q-values in the format \"Q(state, action) = number\"\n",
    "# Use parameters: 100 trajectories per episode, 50 episodes, and learning rate alpha=0.1\n",
    "def run_q_learning(mdp: MDP, num_trajectories=100, num_episodes=50, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run Q-learning on the MDP with multiple episodes of trajectory generation.\n",
    "    \n",
    "    Args:\n",
    "        mdp: MDP object\n",
    "        num_trajectories: Number of trajectories per episode\n",
    "        num_episodes: Number of episodes (iterations of trajectory generation + learning)\n",
    "        alpha: Learning rate\n",
    "    \"\"\"\n",
    "    print(\"\\nRunning Q-Learning...\")\n",
    "    \n",
    "    # Collect trajectories over multiple episodes\n",
    "    all_trajectories = []\n",
    "    for episode in range(num_episodes):\n",
    "        # Generate trajectories using random policy\n",
    "        trajectories = generate_trajectories(mdp, 'random', num_trajectories)\n",
    "        all_trajectories.extend(trajectories)\n",
    "    \n",
    "    # Run Q-learning on all collected trajectories\n",
    "    Q = q_learning(all_trajectories, mdp, alpha=alpha)\n",
    "    \n",
    "    # Print Q-values\n",
    "    print(\"\\nQ-values:\")\n",
    "    for state in mdp.get_states():\n",
    "        for action in mdp.get_actions(state):\n",
    "            print(f\"Q({state}, {action}) = {Q[state][action]:.4f}\")\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Run Q-learning\n",
    "Q_table = run_q_learning(mdp, num_trajectories=100, num_episodes=50, alpha=0.1)\n",
    "\n",
    "# --- End LLM prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P10: Policy implied by Q-values\n",
    "\n",
    "Write a function that extracts a policy form q-values. \n",
    "Apply it to the Q-table obtained at P9. Print out the resulting policy in a readable way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy from Q-Learning:\n",
      "π(SleepingWarg) = Pet\n",
      "π(AngryWarg) = Strike\n",
      "π(FuriousWarg) = Strike\n",
      "π(ApoplecticWarg) = Strike\n",
      "\n",
      "Comparison:\n",
      "State               | Value Iteration | Q-Learning\n",
      "-------------------------------------------------------\n",
      "SleepingWarg       | Pet             | Pet             ✓\n",
      "AngryWarg          | Strike          | Strike          ✓\n",
      "FuriousWarg        | Strike          | Strike          ✓\n",
      "ApoplecticWarg     | Strike          | Strike          ✓\n"
     ]
    }
   ],
   "source": [
    "# --- LLM used: Claude Sonnet 4.5\n",
    "# --- LLM prompt:\n",
    "# Write a function to extract a policy from Q-values. For each state, the policy should \n",
    "# select the action with the highest Q-value. Terminal states should have None as their action. \n",
    "# Print the resulting policy in a readable table format\n",
    "\n",
    "def extract_policy_from_q(Q, mdp: MDP):\n",
    "    \"\"\"\n",
    "    Extract policy from Q-values by choosing action with highest Q-value.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table (nested dict)\n",
    "        mdp: MDP object\n",
    "    \n",
    "    Returns:\n",
    "        Policy dictionary mapping states to actions\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for state in mdp.get_states():\n",
    "        if mdp.is_terminal(state):\n",
    "            policy[state] = None\n",
    "        else:\n",
    "            # Find action with maximum Q-value\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            \n",
    "            for action in mdp.get_actions(state):\n",
    "                if Q[state][action] > best_value:\n",
    "                    best_value = Q[state][action]\n",
    "                    best_action = action\n",
    "            \n",
    "            policy[state] = best_action\n",
    "    \n",
    "    return policy\n",
    "\n",
    "# Extract and print policy from Q-learning\n",
    "print(\"\\nPolicy from Q-Learning:\")\n",
    "q_policy = extract_policy_from_q(Q_table, mdp)\n",
    "for state, action in q_policy.items():\n",
    "    if action is None:\n",
    "        print(f\"π({state}) = Terminal\")\n",
    "    else:\n",
    "        print(f\"π({state}) = {action}\")\n",
    "\n",
    "# Compare with value iteration policy\n",
    "print(\"\\nComparison:\")\n",
    "print(\"State               | Value Iteration | Q-Learning\")\n",
    "print(\"-\" * 55)\n",
    "for state in mdp.get_states():\n",
    "    vi_action = policy.get(state, 'N/A')\n",
    "    ql_action = q_policy.get(state, 'N/A')\n",
    "    match = \"✓\" if vi_action == ql_action else \"✗\"\n",
    "    print(f\"{state:18} | {vi_action:15} | {ql_action:15} {match}\")\n",
    "\n",
    "# --- End LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
