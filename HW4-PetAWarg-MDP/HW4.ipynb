{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Homework 4: Petting a warg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Petting a warg\n",
    "\n",
    "Wargs do not make good pets. They are vicious creatures, populating Middle Earth, the world described by novels of John Ronald Reuel Tolkien. They tend to show up in the worst moment possible. They eat humans, hobbits, elves and wizards (when they can get them).\n",
    "\n",
    "![A warg, getting ready for breakfast w:300px](figures/Gundabad_Wargs.jpg)\n",
    "\n",
    "Your relationship with a warg can be in the following states:\n",
    "```\n",
    "SleepingWarg\n",
    "AngryWarg\n",
    "FuriousWarg\n",
    "ApoplecticWarg\n",
    "Safe\n",
    "Sorry \n",
    "```\n",
    "\n",
    "![tes](figures/WargStates.jpg)\n",
    "\n",
    "Your actions are limited to petting a warg or striking it with your sword. The transitions are described in the following picture. The safe and sorry states are terminal, where no further actions can be taken. Landing into them has the reward +10 and -10 respectively. All other actions have a reward of -1. \n",
    "\n",
    "The discount factor is $\\gamma=0.9$\n",
    "\n",
    "![](figures/PetAWarg.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to solve this homework\n",
    "The following problems you can solve either with the help of an LLM or by hand. \n",
    "\n",
    "* If you are solving by hand, make sure that you add sufficient comments to make sure that the code is understandable. \n",
    "* If you are solving using an LLM, add in form of comments\n",
    "    * the LLM used (at the first use instance)\n",
    "    * the prompt used to elicit the code\n",
    "    * modifications that had to be done to the code \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "# --- LLM used: ChatGPT 4.5\n",
    "# --- LLM prompt\n",
    "# Write a python class to encapsulate the least common multiple algorithm\n",
    "# --- End of LLM prompt\n",
    "```\n",
    "\n",
    "The programming language should be Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1: MDP implementation \n",
    "\n",
    "Write a class to implement an MDP. Do not include value or policy iteration in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    def __init__(self, states, terminal_states, actions, discount_factor):\n",
    "\n",
    "        self.states = states\n",
    "        self.start_state = states[0]\n",
    "        self.actions = actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.terminal_states = terminal_states\n",
    "        # --- LLM used: Sonnet 4.5\n",
    "        # --- LLM prompt: \n",
    "        # What is a way I can implement a set of transition functions without having to pass them into the class\n",
    "        # to ensure I can store the probabiulity and reward in the same structure?\n",
    "        self.transitions = {s: {a: [] for a in actions} for s in states}\n",
    "        \n",
    "    def add_transition(self, state, action, info: tuple[float, str, int]):\n",
    "        probability, next_state, reward = info\n",
    "        self.transitions[state][action].append((probability, next_state, reward))\n",
    "    # --- End of LLM prompt\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return (state in self.terminal_states)\n",
    "    \n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transitions[state][action]\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        return self.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2: Warg as an MDP\n",
    "Implement the WargPettingGame as an MDP using the implementation from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WargPettingGame_MDP():\n",
    "    states = ['SleepingWarg', 'AngryWarg', 'FuriousWarg', 'ApoplecticWarg']\n",
    "    terminal_states = ['Safe', 'Sorry']\n",
    "    actions = ['Strike', 'Pet']\n",
    "    discount_factor = 0.9\n",
    "\n",
    "    game = MDP(states, terminal_states, actions, discount_factor)\n",
    "    # SleepingWarg State\n",
    "    game.add_transition('SleepingWarg', 'Strike', (1.0, 'AngryWarg', -1))\n",
    "    game.add_transition('SleepingWarg', 'Pet', (0.95, 'AngryWarg', -1))\n",
    "    game.add_transition('SleepingWarg', 'Pet', (0.05, 'Safe', 10))\n",
    "    # AngryWarg State\n",
    "    game.add_transition('AngryWarg', 'Strike', (1.0, 'FuriousWarg', -1))\n",
    "    game.add_transition('AngryWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "    # FuriousWarg State\n",
    "    game.add_transition('FuriousWarg', 'Strike', (1.0, 'ApoplecticWarg', -1))\n",
    "    game.add_transition('FuriousWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "    # ApoplecticWarg\n",
    "    game.add_transition('ApoplecticWarg', 'Strike', (0.8, 'Sorry', -10))\n",
    "    game.add_transition('ApoplecticWarg', 'Strike', (0.2, 'Safe', 10))\n",
    "    game.add_transition('ApoplecticWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3: Value iteration\n",
    "\n",
    "Implement the value iteration as a separate function that uses this MDP implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4: Using value iteration\n",
    "Find the V* values of the WargPettingGame using the implementation above. Print out the V* values for each state in the form \n",
    "V(state) == number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5:  Policy extraction\n",
    "\n",
    "Find the policy $\\pi(s)$ from the V values obtained in the previous step. Remember that you need to do one step of expectimax.\n",
    "Print out the policy for each state, in a readable way. Eg. \n",
    "    pi(ApoplecticWarg) = Pet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P6: Policy iteration\n",
    "Implement policy iteration with the MDP as defined above as a separate function.\n",
    "Apply it to the MDP defining the pet the warg game. \n",
    "Print out the resulting policy for each state, in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7: Trajectory sampling\n",
    "Implement a function that generates trajectories in the form of (s,a,r,s') tuples from the MDP for a specific policy. The trajectory ends when it reaches a terminal state. \n",
    "\n",
    "Generate 100 trajectories for a __random__ policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P8: Implement Q-learning \n",
    "\n",
    "Create an implementation of Q-learning which takes the trajectory database and updates a Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P9: Run Q-learning \n",
    "\n",
    "Run your implementation of Q-learning on the warg petting game. Print out the Q values in the form \n",
    "\n",
    "Q(state, action) = number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P10: Policy implied by Q-values\n",
    "\n",
    "Write a function that extracts a policy form q-values. \n",
    "Apply it to the Q-table obtained at P9. Print out the resulting policy in a readable way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
