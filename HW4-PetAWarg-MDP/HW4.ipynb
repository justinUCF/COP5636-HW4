{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Homework 4: Petting a warg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Petting a warg\n",
    "\n",
    "Wargs do not make good pets. They are vicious creatures, populating Middle Earth, the world described by novels of John Ronald Reuel Tolkien. They tend to show up in the worst moment possible. They eat humans, hobbits, elves and wizards (when they can get them).\n",
    "\n",
    "![A warg, getting ready for breakfast w:300px](figures/Gundabad_Wargs.jpg)\n",
    "\n",
    "Your relationship with a warg can be in the following states:\n",
    "```\n",
    "SleepingWarg\n",
    "AngryWarg\n",
    "FuriousWarg\n",
    "ApoplecticWarg\n",
    "Safe\n",
    "Sorry \n",
    "```\n",
    "\n",
    "![tes](figures/WargStates.jpg)\n",
    "\n",
    "Your actions are limited to petting a warg or striking it with your sword. The transitions are described in the following picture. The safe and sorry states are terminal, where no further actions can be taken. Landing into them has the reward +10 and -10 respectively. All other actions have a reward of -1. \n",
    "\n",
    "The discount factor is $\\gamma=0.9$\n",
    "\n",
    "![](figures/PetAWarg.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to solve this homework\n",
    "The following problems you can solve either with the help of an LLM or by hand. \n",
    "\n",
    "* If you are solving by hand, make sure that you add sufficient comments to make sure that the code is understandable. \n",
    "* If you are solving using an LLM, add in form of comments\n",
    "    * the LLM used (at the first use instance)\n",
    "    * the prompt used to elicit the code\n",
    "    * modifications that had to be done to the code \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "# --- LLM used: ChatGPT 4.5\n",
    "# --- LLM prompt\n",
    "# Write a python class to encapsulate the least common multiple algorithm\n",
    "# --- End of LLM prompt\n",
    "```\n",
    "\n",
    "The programming language should be Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1: MDP implementation \n",
    "\n",
    "Write a class to implement an MDP. Do not include value or policy iteration in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    def __init__(self, states, terminal_states, actions, discount_factor):\n",
    "\n",
    "        self.states = states\n",
    "        self.start_state = states[0]\n",
    "        self.actions = actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.terminal_states = terminal_states\n",
    "        # --- LLM used: Sonnet 4.5\n",
    "        # --- LLM prompt: \n",
    "        # What is a way I can implement a set of transition functions without having to pass them into the class\n",
    "        # to ensure I can store the probabiulity and reward in the same structure?\n",
    "        self.transitions = {s: {a: [] for a in actions} for s in states}\n",
    "        \n",
    "    def add_transition(self, state, action, info: tuple[float, str, int]):\n",
    "        probability, next_state, reward = info\n",
    "        self.transitions[state][action].append((probability, next_state, reward))\n",
    "    # --- End of LLM prompt\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return (state in self.terminal_states)\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transitions[state][action]\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        return self.actions\n",
    "    \n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor\n",
    "    \n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "    \n",
    "    def get_terminal_states(self):\n",
    "        return self.terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2: Warg as an MDP\n",
    "Implement the WargPettingGame as an MDP using the implementation from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WargPettingGame_MDP():\n",
    "    states = ['SleepingWarg', 'AngryWarg', 'FuriousWarg', 'ApoplecticWarg']\n",
    "    terminal_states = ['Safe', 'Sorry']\n",
    "    actions = ['Strike', 'Pet']\n",
    "    discount_factor = 0.9\n",
    "\n",
    "    game = MDP(states, terminal_states, actions, discount_factor)\n",
    "    # SleepingWarg State\n",
    "    game.add_transition('SleepingWarg', 'Strike', (1.0, 'AngryWarg', -1))\n",
    "    game.add_transition('SleepingWarg', 'Pet', (0.95, 'AngryWarg', -1))\n",
    "    game.add_transition('SleepingWarg', 'Pet', (0.05, 'Safe', 10))\n",
    "    # AngryWarg State\n",
    "    game.add_transition('AngryWarg', 'Strike', (1.0, 'FuriousWarg', -1))\n",
    "    game.add_transition('AngryWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "    # FuriousWarg State\n",
    "    game.add_transition('FuriousWarg', 'Strike', (1.0, 'ApoplecticWarg', -1))\n",
    "    game.add_transition('FuriousWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "    # ApoplecticWarg\n",
    "    game.add_transition('ApoplecticWarg', 'Strike', (0.8, 'Sorry', -10))\n",
    "    game.add_transition('ApoplecticWarg', 'Strike', (0.2, 'Safe', 10))\n",
    "    game.add_transition('ApoplecticWarg', 'Pet', (1.0, 'Sorry', -10))\n",
    "\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3: Value iteration\n",
    "\n",
    "Implement the value iteration as a separate function that uses this MDP implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: MDP, iterations=10000, convergence=0.0001):\n",
    "    # Inilialize V for every state to 0.0\n",
    "    V = {s: 0.0 for s in mdp.get_states()} \n",
    "    for terminal_state in mdp.get_terminal_states():\n",
    "        V[terminal_state] = 0.0\n",
    "    for i in range(iterations):\n",
    "        V_next = V.copy() # V_k+1\n",
    "        delta = 0 # Difference between iterations for convergence\n",
    "\n",
    "        # --- LLM used: Sonnet 4.5\n",
    "        # --- LLM prompt\n",
    "        # How can I loop through each of the possible transitions for a given action and compute the q value for \n",
    "        # value iteration. Please review these slides from my class to understand the bellman equation we used.\n",
    "\n",
    "        # For each non-terminal state\n",
    "        for state in mdp.get_states():\n",
    "\n",
    "            # Compute Q*(s,a) for each action\n",
    "            action_values = []\n",
    "            for action in mdp.get_actions(state):\n",
    "                q = 0\n",
    "\n",
    "                # Q*(s,a) = Σ T(s,a,s') * [R(s,a,s') + γ * V_k(s')]\n",
    "                transitions = mdp.get_transitions(state, action)\n",
    "                for info in transitions:\n",
    "                    probability, next_state, reward = info\n",
    "                    q += probability * (reward + mdp.get_discount_factor() * V[next_state])\n",
    "\n",
    "                action_values.append(q)\n",
    "\n",
    "            # V_{k+1}(s) = max_a Q*(s,a) (Bellman update)\n",
    "            if action_values:\n",
    "                V_next[state] = max(action_values)\n",
    "                delta = max(delta, abs(V_next[state] - V[state]))\n",
    "\n",
    "        V = V_next\n",
    "        # --- End of LLM prompt\n",
    "        if delta < convergence:\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4: Using value iteration\n",
    "Find the V* values of the WargPettingGame using the implementation above. Print out the V* values for each state in the form \n",
    "V(state) == number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(SleepingWarg) = -6.2298\n",
      "V(AngryWarg) = -6.760000000000001\n",
      "V(FuriousWarg) = -6.4\n",
      "V(ApoplecticWarg) = -6.0\n",
      "V(Safe) = 0.0\n",
      "V(Sorry) = 0.0\n"
     ]
    }
   ],
   "source": [
    "mdp = WargPettingGame_MDP()\n",
    "V_star = value_iteration(mdp)\n",
    "for state, action in V_star.items():\n",
    "    print(f\"V({state}) = {action}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5:  Policy extraction\n",
    "\n",
    "Find the policy $\\pi(s)$ from the V values obtained in the previous step. Remember that you need to do one step of expectimax.\n",
    "Print out the policy for each state, in a readable way. Eg. \n",
    "    pi(ApoplecticWarg) = Pet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi(SleepingWarg) = Pet\n",
      "pi(AngryWarg) = Strike\n",
      "pi(FuriousWarg) = Strike\n",
      "pi(ApoplecticWarg) = Strike\n"
     ]
    }
   ],
   "source": [
    "def extract_policy(mdp: MDP, V: dict):\n",
    "    policy = {}\n",
    "    \n",
    "    for state in mdp.get_states():\n",
    "        policy[state] = None\n",
    "            \n",
    "        \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        # Try each action\n",
    "        for action in mdp.get_actions(state):\n",
    "            q_value = 0\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "            \n",
    "            # Compute expected value for this action\n",
    "            for prob, next_state, reward in transitions:\n",
    "                q_value += prob * (reward + mdp.get_discount_factor() * V[next_state])\n",
    "            \n",
    "            # Update best action if this is better\n",
    "            if q_value > best_value:\n",
    "                best_value = q_value\n",
    "                best_action = action\n",
    "        \n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy\n",
    "\n",
    "policy = extract_policy(mdp, V_star)\n",
    "for state, action in policy.items():\n",
    "    print(f\"pi({state}) = {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P6: Policy iteration\n",
    "Implement policy iteration with the MDP as defined above as a separate function.\n",
    "Apply it to the MDP defining the pet the warg game. \n",
    "Print out the resulting policy for each state, in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SleepingWarg': 'Pet', 'AngryWarg': 'Strike', 'FuriousWarg': 'Strike', 'ApoplecticWarg': 'Strike'}\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(mdp, policy, convergence=0.0001):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7: Trajectory sampling\n",
    "Implement a function that generates trajectories in the form of (s,a,r,s') tuples from the MDP for a specific policy. The trajectory ends when it reaches a terminal state. \n",
    "\n",
    "Generate 100 trajectories for a __random__ policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P8: Implement Q-learning \n",
    "\n",
    "Create an implementation of Q-learning which takes the trajectory database and updates a Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P9: Run Q-learning \n",
    "\n",
    "Run your implementation of Q-learning on the warg petting game. Print out the Q values in the form \n",
    "\n",
    "Q(state, action) = number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P10: Policy implied by Q-values\n",
    "\n",
    "Write a function that extracts a policy form q-values. \n",
    "Apply it to the Q-table obtained at P9. Print out the resulting policy in a readable way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
